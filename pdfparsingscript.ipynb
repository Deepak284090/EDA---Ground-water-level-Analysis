{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf2e81b2-8138-4d64-82fd-7764a793bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a375d975-56c0-4bc3-8518-f77429bd9270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.5-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.5-cp39-abi3-win_amd64.whl (18.7 MB)\n",
      "   ---------------------------------------- 0.0/18.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/18.7 MB 7.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.6/18.7 MB 7.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 4.2/18.7 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 6.0/18.7 MB 7.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 8.1/18.7 MB 8.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 10.2/18.7 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 11.8/18.7 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 14.2/18.7 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 16.0/18.7 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.1/18.7 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.7/18.7 MB 8.7 MB/s  0:00:02\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.5\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59bafc6b-b71a-4a8f-b097-69b774a9b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"january_wl_1994-2024-compressed.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca45b248-1139-4649-bbcc-efa5eb1a99e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening PDF with PyMuPDF (fitz): january_wl_1994-2024-compressed.pdf...\n",
      "Total pages found: 10242\n",
      "Output files 'good_data.csv' and 'error_data.csv' created.\n",
      "Processing page 500 of 10242...\n",
      "Processing page 1000 of 10242...\n",
      "Processing page 1500 of 10242...\n",
      "Processing page 2000 of 10242...\n",
      "Processing page 2500 of 10242...\n",
      "Processing page 3000 of 10242...\n",
      "Processing page 3500 of 10242...\n",
      "Processing page 4000 of 10242...\n",
      "Processing page 4500 of 10242...\n",
      "Processing page 5000 of 10242...\n",
      "Processing page 5500 of 10242...\n",
      "Processing page 6000 of 10242...\n",
      "Processing page 6500 of 10242...\n",
      "Processing page 7000 of 10242...\n",
      "Processing page 7500 of 10242...\n",
      "Processing page 8000 of 10242...\n",
      "Processing page 8500 of 10242...\n",
      "Processing page 9000 of 10242...\n",
      "Processing page 9500 of 10242...\n",
      "Processing page 10000 of 10242...\n",
      "Processing page 10242 of 10242...\n",
      "\n",
      "--- Extraction Complete ---\n",
      "Good rows written: 383226\n",
      "Bad/partial rows written: 5866\n",
      "Size of 'good_data.csv': 25.88 MB\n",
      "Size of 'error_data.csv': 0.45 MB\n",
      "PDF document closed.\n",
      "\n",
      "Loading good data from 'good_data.csv' into DataFrame...\n",
      "Processing data chunk 1...\n",
      "Combining all processed chunks into one DataFrame...\n",
      "\n",
      "--- DataFrame is Ready for Analysis ---\n",
      "         STATE_UT               DISTRICT        BLOCK             VILLAGE  \\\n",
      "0  Andhra Pradesh  Alluri Sitharama Raju  Addateegala         Addateegala   \n",
      "1  Andhra Pradesh  Alluri Sitharama Raju  Addateegala  Mallavaram Mammilu   \n",
      "2  Andhra Pradesh  Alluri Sitharama Raju  Addateegala           Rayapalli   \n",
      "3  Andhra Pradesh  Alluri Sitharama Raju  Addateegala     Veerbhadrapuram   \n",
      "4  Andhra Pradesh  Alluri Sitharama Raju  Ananthagiri          Anantagiri   \n",
      "\n",
      "   LATITUDE  LONGITUDE       Date  WL(mbgl)  \n",
      "0  17.46330    82.0271 2024-01-01       9.0  \n",
      "1  17.50320    82.0124 2024-01-01       4.0  \n",
      "2  17.57208    82.0085 2024-01-01       1.9  \n",
      "3  17.43790    82.0740 2024-01-01       1.4  \n",
      "4  18.23880    83.0011 2024-01-01       5.3  \n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 383167 entries, 0 to 383166\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   STATE_UT   383167 non-null  object        \n",
      " 1   DISTRICT   383167 non-null  object        \n",
      " 2   BLOCK      383167 non-null  object        \n",
      " 3   VILLAGE    383167 non-null  object        \n",
      " 4   LATITUDE   383167 non-null  float64       \n",
      " 5   LONGITUDE  383167 non-null  float64       \n",
      " 6   Date       383167 non-null  datetime64[ns]\n",
      " 7   WL(mbgl)   383167 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(3), object(4)\n",
      "memory usage: 23.4+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osadm\\AppData\\Local\\Temp\\ipykernel_3644\\1915151008.py:158: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['Date'] = pd.to_datetime(chunk['Date'], errors='coerce') # Flexible date parsing\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "pdf_path = path\n",
    "good_csv_path = \"good_data.csv\"\n",
    "error_csv_path = \"error_data.csv\"\n",
    "\n",
    "# The 8-column header\n",
    "csv_header = \"STATE_UT,DISTRICT,BLOCK,VILLAGE,LATITUDE,LONGITUDE,Date,WL(mbgl)\"\n",
    "\n",
    "# The \"anchor\" list, as you suggested.\n",
    "# This is our set of \"sentinel\" values.\n",
    "STATES_AND_UTS = {\n",
    "    'Andhra Pradesh', 'Arunachal Pradesh', 'Assam', 'Bihar', 'Chhattisgarh',\n",
    "    'Goa', 'Gujarat', 'Haryana', 'Himachal Pradesh', 'Jharkhand', 'Karnataka',\n",
    "    'Kerala', 'Madhya Pradesh', 'Maharashtra', 'Manipur', 'Meghalaya',\n",
    "    'Mizoram', 'Nagaland', 'Odisha', 'Punjab', 'Rajasthan', 'Sikkim',\n",
    "    'Tamil Nadu', 'Telangana', 'Tripura', 'Uttar Pradesh', 'Uttarakhand',\n",
    "    'West Bengal', 'Andaman and Nicobar Islands', 'Chandigarh',\n",
    "    'Dadra and Nagar Haveli and Daman and Diu', 'Delhi', 'Jammu and Kashmir',\n",
    "    'Ladakh', 'Lakshadweep', 'Puducherry'\n",
    "}\n",
    "\n",
    "# Junk lines to filter out\n",
    "junk_lines = {\n",
    "    'STATE_UT', 'DISTRICT', 'BLOCK', 'VILLAGE', 'LATITUDE', 'LONGITUDE',\n",
    "    'Date', 'WL(mbgl)',\n",
    "    'January month Depth to Water Level (In mbgl) Data of Unconfined Aquifer',\n",
    "    ''\n",
    "}\n",
    "\n",
    "# --- 2. Main Extraction Loop (Anchor Logic) ---\n",
    "print(f\"Opening PDF with PyMuPDF (fitz): {pdf_path}...\")\n",
    "doc = None\n",
    "current_row = []  # This will store the cells for one row\n",
    "good_rows_written = 0\n",
    "bad_rows_written = 0\n",
    "\n",
    "try:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = doc.page_count\n",
    "    print(f\"Total pages found: {total_pages}\")\n",
    "\n",
    "    # Open BOTH output files\n",
    "    with open(good_csv_path, 'w', encoding='utf-8') as f_good, \\\n",
    "         open(error_csv_path, 'w', encoding='utf-8') as f_error:\n",
    "\n",
    "        # Write the header to both files\n",
    "        f_good.write(csv_header + '\\n')\n",
    "        f_error.write(csv_header + '\\n')\n",
    "        print(f\"Output files '{good_csv_path}' and '{error_csv_path}' created.\")\n",
    "\n",
    "        # Loop through every page\n",
    "        for i in range(total_pages):\n",
    "\n",
    "            if (i + 1) % 500 == 0 or (i + 1) == total_pages:\n",
    "                print(f\"Processing page {i + 1} of {total_pages}...\")\n",
    "\n",
    "            page = doc.load_page(i)\n",
    "            text = page.get_text(\"text\")\n",
    "\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            lines = text.split('\\n')\n",
    "\n",
    "            for line in lines:\n",
    "                cleaned_line = line.strip()\n",
    "\n",
    "                # 1. Skip all known junk\n",
    "                if not cleaned_line or cleaned_line in junk_lines:\n",
    "                    continue\n",
    "\n",
    "                # 2. Check if the line is an ANCHOR\n",
    "                if cleaned_line in STATES_AND_UTS:\n",
    "                    # This is the start of a new row.\n",
    "                    # First, we must handle the 'current_row' we were building.\n",
    "\n",
    "                    if current_row: # If it's not empty\n",
    "                        # Check if the row we *were* building is complete\n",
    "                        if len(current_row) == 8:\n",
    "                            f_good.write(','.join(current_row) + '\\n')\n",
    "                            good_rows_written += 1\n",
    "                        else:\n",
    "                            # It's an incomplete/corrupt row\n",
    "                            f_error.write(','.join(current_row) + '\\n')\n",
    "                            bad_rows_written += 1\n",
    "\n",
    "                    # 3. Start the NEW row, anchored by the state\n",
    "                    current_row = [cleaned_line]\n",
    "\n",
    "                # 4. If it's NOT an anchor, add it to the current row\n",
    "                else:\n",
    "                    # Only add if we've already found an anchor\n",
    "                    if current_row:\n",
    "                        current_row.append(cleaned_line)\n",
    "\n",
    "                        # Optimization: If we just added the 8th item,\n",
    "                        # write it out immediately and clear the row.\n",
    "                        # This handles perfectly formatted rows quickly.\n",
    "                        if len(current_row) == 8:\n",
    "                            f_good.write(','.join(current_row) + '\\n')\n",
    "                            good_rows_written += 1\n",
    "                            current_row = [] # Reset for the next anchor\n",
    "                    # else:\n",
    "                        # This is junk data appearing *before* the first\n",
    "                        # state anchor on a page. We ignore it.\n",
    "\n",
    "    # --- End of Loop ---\n",
    "    # Handle the very last row left in the buffer\n",
    "    if current_row:\n",
    "        if len(current_row) == 8:\n",
    "            f_good.write(','.join(current_row) + '\\n')\n",
    "            good_rows_written += 1\n",
    "        else:\n",
    "            f_error.write(','.join(current_row) + '\\n')\n",
    "            bad_rows_written += 1\n",
    "\n",
    "    print(\"\\n--- Extraction Complete ---\")\n",
    "    print(f\"Good rows written: {good_rows_written}\")\n",
    "    print(f\"Bad/partial rows written: {bad_rows_written}\")\n",
    "\n",
    "    good_size = os.path.getsize(good_csv_path) / (1024*1024)\n",
    "    error_size = os.path.getsize(error_csv_path) / (1024*1024)\n",
    "\n",
    "    print(f\"Size of '{good_csv_path}': {good_size:.2f} MB\")\n",
    "    print(f\"Size of '{error_csv_path}': {error_size:.2f} MB\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {pdf_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction: {e}\")\n",
    "finally:\n",
    "    if doc:\n",
    "        doc.close()\n",
    "    print(\"PDF document closed.\")\n",
    "\n",
    "# --- 3. Load the GOOD data for analysis ---\n",
    "if good_rows_written > 0:\n",
    "    print(f\"\\nLoading good data from '{good_csv_path}' into DataFrame...\")\n",
    "\n",
    "    try:\n",
    "        # We still use chunks, as the good file might be huge\n",
    "        chunk_iterator = pd.read_csv(good_csv_path, chunksize=1000000, on_bad_lines='skip')\n",
    "\n",
    "        list_of_dataframes = []\n",
    "        for i, chunk in enumerate(chunk_iterator):\n",
    "            print(f\"Processing data chunk {i+1}...\")\n",
    "\n",
    "            if chunk.empty:\n",
    "                continue\n",
    "\n",
    "            # Convert types (no more column hacks needed)\n",
    "            chunk['WL(mbgl)'] = pd.to_numeric(chunk['WL(mbgl)'], errors='coerce')\n",
    "            chunk['Date'] = pd.to_datetime(chunk['Date'], errors='coerce') # Flexible date parsing\n",
    "            chunk['LATITUDE'] = pd.to_numeric(chunk['LATITUDE'], errors='coerce')\n",
    "            chunk['LONGITUDE'] = pd.to_numeric(chunk['LONGITUDE'], errors='coerce')\n",
    "\n",
    "            list_of_dataframes.append(chunk)\n",
    "\n",
    "        if list_of_dataframes:\n",
    "            print(\"Combining all processed chunks into one DataFrame...\")\n",
    "            df = pd.concat(list_of_dataframes)\n",
    "\n",
    "            print(\"\\n--- DataFrame is Ready for Analysis ---\")\n",
    "            print(df.head())\n",
    "            print(\"\\nDataFrame Info:\")\n",
    "            df.info()\n",
    "        else:\n",
    "            print(\"No valid data chunks were loaded.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the CSV: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo good rows were extracted. The DataFrame is empty.\")\n",
    "    print(f\"Please check '{error_csv_path}' to see what data was found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb60e46-95ce-4acf-b343-e41112410e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening PDF with PyMuPDF (fitz): january_wl_1994-2024-compressed.pdf...\n",
      "Total pages found: 10242\n",
      "Output files 'good_data_v3.csv' and 'error_data_v3.csv' created.\n",
      "Processing page 500 of 10242...\n",
      "Processing page 1000 of 10242...\n",
      "Processing page 1500 of 10242...\n",
      "Processing page 2000 of 10242...\n",
      "Processing page 2500 of 10242...\n",
      "Processing page 3000 of 10242...\n",
      "Processing page 3500 of 10242...\n",
      "Processing page 4000 of 10242...\n",
      "Processing page 4500 of 10242...\n",
      "Processing page 5000 of 10242...\n",
      "Processing page 5500 of 10242...\n",
      "Processing page 6000 of 10242...\n",
      "Processing page 6500 of 10242...\n",
      "Processing page 7000 of 10242...\n",
      "Processing page 7500 of 10242...\n",
      "Processing page 8000 of 10242...\n",
      "Processing page 8500 of 10242...\n",
      "Processing page 9000 of 10242...\n",
      "Processing page 9500 of 10242...\n",
      "Processing page 10000 of 10242...\n",
      "Processing page 10242 of 10242...\n",
      "\n",
      "--- Extraction Complete ---\n",
      "Good 'parent' rows written: 195635\n",
      "Good 'child' rows fixed: 2761 (Thanks to new validation!)\n",
      "Total good rows: 198396\n",
      "Bad/partial rows logged: 768768\n",
      "PDF document closed.\n",
      "\n",
      "Loading good data from 'good_data_v3.csv' into DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osadm\\AppData\\Local\\Temp\\ipykernel_3644\\2810973768.py:166: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "C:\\Users\\osadm\\AppData\\Local\\Temp\\ipykernel_3644\\2810973768.py:171: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['Date'] = pd.to_datetime(chunk['Date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data chunk 1...\n",
      "Combining all processed chunks into one DataFrame...\n",
      "\n",
      "--- DataFrame is Ready for Analysis ---\n",
      "         STATE_UT               DISTRICT        BLOCK             VILLAGE  \\\n",
      "0  Andhra Pradesh  Alluri Sitharama Raju  Addateegala         Addateegala   \n",
      "1  Andhra Pradesh  Alluri Sitharama Raju  Addateegala  Mallavaram Mammilu   \n",
      "2  Andhra Pradesh  Alluri Sitharama Raju  Addateegala           Rayapalli   \n",
      "3  Andhra Pradesh  Alluri Sitharama Raju  Addateegala     Veerbhadrapuram   \n",
      "4  Andhra Pradesh  Alluri Sitharama Raju  Ananthagiri          Anantagiri   \n",
      "\n",
      "   LATITUDE  LONGITUDE       Date  WL(mbgl)  \n",
      "0  17.46330    82.0271 2024-01-01       9.0  \n",
      "1  17.50320    82.0124 2024-01-01       4.0  \n",
      "2  17.57208    82.0085 2024-01-01       1.9  \n",
      "3  17.43790    82.0740 2024-01-01       1.4  \n",
      "4  18.23880    83.0011 2024-01-01       5.3  \n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 198368 entries, 0 to 198367\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   STATE_UT   198368 non-null  object        \n",
      " 1   DISTRICT   198368 non-null  object        \n",
      " 2   BLOCK      198368 non-null  object        \n",
      " 3   VILLAGE    198368 non-null  object        \n",
      " 4   LATITUDE   198362 non-null  float64       \n",
      " 5   LONGITUDE  193198 non-null  float64       \n",
      " 6   Date       195742 non-null  datetime64[ns]\n",
      " 7   WL(mbgl)   195742 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(3), object(4)\n",
      "memory usage: 12.1+ MB\n",
      "\n",
      "--- Example of your 'Pivot' idea ---\n",
      "Date                                                                                    1994-01-01  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    1994-01-13  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    1994-01-15  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    1994-01-16  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    1994-01-17  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    1994-01-18  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    1994-01-19  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    1994-01-20  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    1994-01-22  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    1994-01-25  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    ...  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE  ...   \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271    ...   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124    ...   \n",
      "                                                 Rayapalli          17.57208 82.0085    ...   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740    ...   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011    ...   \n",
      "\n",
      "Date                                                                                    2024-03-01  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    2024-04-01  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    2024-05-01  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    2024-06-01  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    2024-07-01  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    2024-08-01  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    2024-09-01  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    2024-10-01  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    2024-11-01  \\\n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE               \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN   \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN   \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN   \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN   \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN   \n",
      "\n",
      "Date                                                                                    2024-12-01  \n",
      "STATE_UT       DISTRICT              BLOCK       VILLAGE            LATITUDE LONGITUDE              \n",
      "Andhra Pradesh Alluri Sitharama Raju Addateegala Addateegala        17.46330 82.0271           NaN  \n",
      "                                                 Mallavaram Mammilu 17.50320 82.0124           NaN  \n",
      "                                                 Rayapalli          17.57208 82.0085           NaN  \n",
      "                                                 Veerbhadrapuram    17.43790 82.0740           NaN  \n",
      "                                     Ananthagiri Anantagiri         18.23880 83.0011           NaN  \n",
      "\n",
      "[5 rows x 775 columns]\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import re  # <-- We need this for date validation\n",
    "\n",
    "# --- 1. Helper Validation Functions ---\n",
    "\n",
    "def is_date(s):\n",
    "    \"\"\"Checks if a string loosely looks like a date (e.g., 10-04-24 or 2024-01-01)\"\"\"\n",
    "    # This regex checks for dd-mm-yy, dd-mm-yyyy, yyyy-mm-dd etc.\n",
    "    if re.match(r'^\\d{2}-\\d{2}-\\d{2,4}$', s) or re.match(r'^\\d{4}-\\d{2}-\\d{2}$', s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_number(s):\n",
    "    \"\"\"Checks if a string can be converted to a float.\"\"\"\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "# --- 2. Configuration ---\n",
    "pdf_path = path\n",
    "good_csv_path = \"good_data_v3.csv\"\n",
    "error_csv_path = \"error_data_v3.csv\"\n",
    "\n",
    "csv_header = \"STATE_UT,DISTRICT,BLOCK,VILLAGE,LATITUDE,LONGITUDE,Date,WL(mbgl)\"\n",
    "\n",
    "STATES_AND_UTS = {\n",
    "    'Andhra Pradesh', 'Arunachal Pradesh', 'Assam', 'Bihar', 'Chhattisgarh',\n",
    "    'Goa', 'Gujarat', 'Haryana', 'Himachal Pradesh', 'Jharkhand', 'Karnataka',\n",
    "    'Kerala', 'Madhya Pradesh', 'Maharashtra', 'Manipur', 'Meghalaya',\n",
    "    'Mizoram', 'Nagaland', 'Odisha', 'Punjab', 'Rajasthan', 'Sikkim',\n",
    "    'Tamil Nadu', 'Telangana', 'Tripura', 'Uttar Pradesh', 'Uttarakhand',\n",
    "    'West Bengal', 'Andaman and Nicobar Islands', 'Chandigarh',\n",
    "    'Dadra and Nagar Haveli and Daman and Diu', 'Delhi', 'Jammu and Kashmir',\n",
    "    'Ladakh', 'Lakshadweep', 'Puducherry'\n",
    "}\n",
    "\n",
    "junk_lines = {\n",
    "    'STATE_UT', 'DISTRICT', 'BLOCK', 'VILLAGE', 'LATITUDE', 'LONGITUDE',\n",
    "    'Date', 'WL(mbgl)',\n",
    "    'January month Depth to Water Level (In mbgl) Data of Unconfined Aquifer',\n",
    "    ''\n",
    "}\n",
    "\n",
    "# --- 3. Main Extraction Loop (With Validation) ---\n",
    "print(f\"Opening PDF with PyMuPDF (fitz): {pdf_path}...\")\n",
    "doc = None\n",
    "good_rows_written = 0\n",
    "bad_rows_written = 0\n",
    "child_rows_fixed = 0\n",
    "\n",
    "last_known_location = []  # Memory: [STATE, ... , LONGITUDE]\n",
    "temp_buffer = []          # Collects lines\n",
    "\n",
    "try:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = doc.page_count\n",
    "    print(f\"Total pages found: {total_pages}\")\n",
    "\n",
    "    with open(good_csv_path, 'w', encoding='utf-8') as f_good, \\\n",
    "         open(error_csv_path, 'w', encoding='utf-8') as f_error:\n",
    "\n",
    "        f_good.write(csv_header + '\\n')\n",
    "        f_error.write(\"REASON,DATA\\n\")\n",
    "        print(f\"Output files '{good_csv_path}' and '{error_csv_path}' created.\")\n",
    "\n",
    "        for i in range(total_pages):\n",
    "\n",
    "            if (i + 1) % 500 == 0 or (i + 1) == total_pages:\n",
    "                print(f\"Processing page {i + 1} of {total_pages}...\")\n",
    "\n",
    "            page = doc.load_page(i)\n",
    "            text = page.get_text(\"text\")\n",
    "\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            lines = text.split('\\n')\n",
    "\n",
    "            for line in lines:\n",
    "                cleaned_line = line.strip()\n",
    "\n",
    "                if not cleaned_line or cleaned_line in junk_lines:\n",
    "                    continue\n",
    "\n",
    "                temp_buffer.append(cleaned_line)\n",
    "\n",
    "                # --- DECISION LOGIC ---\n",
    "\n",
    "                # Case 1: Buffer starts with a State Anchor\n",
    "                if temp_buffer[0] in STATES_AND_UTS:\n",
    "                    if len(temp_buffer) == 8:\n",
    "                        # Full 8-item parent row.\n",
    "                        f_good.write(','.join(temp_buffer) + '\\n')\n",
    "                        last_known_location = temp_buffer[:6] # Save location\n",
    "                        temp_buffer = [] # Clear buffer\n",
    "                        good_rows_written += 1\n",
    "                    elif len(temp_buffer) > 8:\n",
    "                        # Error: Row is too long\n",
    "                        f_error.write(f\"OVERFLOW_PARENT_ROW,{','.join(temp_buffer)}\\n\")\n",
    "                        temp_buffer = []\n",
    "                        last_known_location = [] # Reset memory, it's corrupt\n",
    "                        bad_rows_written += 1\n",
    "\n",
    "                # Case 2: Buffer does NOT start with a state\n",
    "                else:\n",
    "                    # We can't do anything without memory\n",
    "                    if not last_known_location:\n",
    "                        f_error.write(f\"NO_MEMORY,{','.join(temp_buffer)}\\n\")\n",
    "                        temp_buffer = []\n",
    "                        bad_rows_written += 1\n",
    "                        continue # Move to next line\n",
    "\n",
    "                    # We have memory, now check the buffer length\n",
    "                    if len(temp_buffer) == 2:\n",
    "                        # --- THIS IS THE NEW CHECK ---\n",
    "                        if is_date(temp_buffer[0]) and is_number(temp_buffer[1]):\n",
    "                            # It's a valid child row!\n",
    "                            full_row = last_known_location + temp_buffer\n",
    "                            f_good.write(','.join(full_row) + '\\n')\n",
    "                            child_rows_fixed += 1\n",
    "                        else:\n",
    "                            # It's 2 items, but NOT (Date, WL)\n",
    "                            # This is the 'Kishorinagar, 10-04-24' case\n",
    "                            f_error.write(f\"INVALID_CHILD_ROW,{','.join(temp_buffer)}\\n\")\n",
    "                            bad_rows_written += 1\n",
    "\n",
    "                        temp_buffer = [] # Clear buffer in both cases\n",
    "\n",
    "                    elif len(temp_buffer) > 2:\n",
    "                        # Error: Child row is too long\n",
    "                        f_error.write(f\"OVERFLOW_CHILD_ROW,{','.join(temp_buffer)}\\n\")\n",
    "                        temp_buffer = []\n",
    "                        bad_rows_written += 1\n",
    "\n",
    "    # --- End of Loop ---\n",
    "    if temp_buffer:\n",
    "        f_error.write(f\"PARTIAL_END_OF_FILE,{','.join(temp_buffer)}\\n\")\n",
    "        bad_rows_written += 1\n",
    "\n",
    "    print(\"\\n--- Extraction Complete ---\")\n",
    "    print(f\"Good 'parent' rows written: {good_rows_written}\")\n",
    "    print(f\"Good 'child' rows fixed: {child_rows_fixed} (Thanks to new validation!)\")\n",
    "    print(f\"Total good rows: {good_rows_written + child_rows_fixed}\")\n",
    "    print(f\"Bad/partial rows logged: {bad_rows_written}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction: {e}\")\n",
    "finally:\n",
    "    if doc:\n",
    "        doc.close()\n",
    "    print(\"PDF document closed.\")\n",
    "\n",
    "# --- 4. Load the GOOD data for analysis ---\n",
    "if (good_rows_written + child_rows_fixed) > 0:\n",
    "    print(f\"\\nLoading good data from '{good_csv_path}' into DataFrame...\")\n",
    "\n",
    "    try:\n",
    "        chunk_iterator = pd.read_csv(good_csv_path, chunksize=1000000, on_bad_lines='skip')\n",
    "        list_of_dataframes = []\n",
    "\n",
    "        for i, chunk in enumerate(chunk_iterator):\n",
    "            print(f\"Processing data chunk {i+1}...\")\n",
    "            if chunk.empty: continue\n",
    "\n",
    "            chunk['WL(mbgl)'] = pd.to_numeric(chunk['WL(mbgl)'], errors='coerce')\n",
    "            chunk['Date'] = pd.to_datetime(chunk['Date'], errors='coerce')\n",
    "            chunk['LATITUDE'] = pd.to_numeric(chunk['LATITUDE'], errors='coerce')\n",
    "            chunk['LONGITUDE'] = pd.to_numeric(chunk['LONGITUDE'], errors='coerce')\n",
    "\n",
    "            list_of_dataframes.append(chunk)\n",
    "\n",
    "        if list_of_dataframes:\n",
    "            print(\"Combining all processed chunks into one DataFrame...\")\n",
    "            df = pd.concat(list_of_dataframes)\n",
    "\n",
    "            print(\"\\n--- DataFrame is Ready for Analysis ---\")\n",
    "            print(df.head())\n",
    "            print(\"\\nDataFrame Info:\")\n",
    "            df.info()\n",
    "\n",
    "            # --- Your Pivot Idea ---\n",
    "            print(\"\\n--- Example of your 'Pivot' idea ---\")\n",
    "            df_pivoted = df.pivot_table(\n",
    "                index=['STATE_UT', 'DISTRICT', 'BLOCK', 'VILLAGE', 'LATITUDE', 'LONGITUDE'],\n",
    "                columns='Date',\n",
    "                values='WL(mbgl)',\n",
    "                aggfunc='last'\n",
    "            )\n",
    "            print(df_pivoted.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the CSV: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo good rows were extracted. The DataFrame is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6890966-a283-402b-92f6-ae33f42b64eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e396c5-90a1-4201-a7de-4bdbb2d68146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading good data from 'good_data.csv' into DataFrame using chunks...\n",
      "Processing data chunk 1...\n",
      "Combining all processed chunks into one DataFrame...\n",
      "\n",
      "--- DataFrame is Ready for Analysis ---\n",
      "         STATE_UT               DISTRICT        BLOCK             VILLAGE  \\\n",
      "0  Andhra Pradesh  Alluri Sitharama Raju  Addateegala         Addateegala   \n",
      "1  Andhra Pradesh  Alluri Sitharama Raju  Addateegala  Mallavaram Mammilu   \n",
      "2  Andhra Pradesh  Alluri Sitharama Raju  Addateegala           Rayapalli   \n",
      "3  Andhra Pradesh  Alluri Sitharama Raju  Addateegala     Veerbhadrapuram   \n",
      "4  Andhra Pradesh  Alluri Sitharama Raju  Ananthagiri          Anantagiri   \n",
      "\n",
      "   LATITUDE  LONGITUDE       Date  WL(mbgl)  \n",
      "0  17.46330    82.0271 2024-01-01       9.0  \n",
      "1  17.50320    82.0124 2024-01-01       4.0  \n",
      "2  17.57208    82.0085 2024-01-01       1.9  \n",
      "3  17.43790    82.0740 2024-01-01       1.4  \n",
      "4  18.23880    83.0011 2024-01-01       5.3  \n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 383167 entries, 0 to 383166\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   STATE_UT   383167 non-null  object        \n",
      " 1   DISTRICT   383167 non-null  object        \n",
      " 2   BLOCK      383167 non-null  object        \n",
      " 3   VILLAGE    383167 non-null  object        \n",
      " 4   LATITUDE   383167 non-null  float64       \n",
      " 5   LONGITUDE  383167 non-null  float64       \n",
      " 6   Date       383167 non-null  datetime64[ns]\n",
      " 7   WL(mbgl)   383167 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(3), object(4)\n",
      "memory usage: 23.4+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osadm\\AppData\\Local\\Temp\\ipykernel_3644\\2563798867.py:44: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['Date'] = pd.to_datetime(chunk['Date'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# --- This is the correct way to load your CSV ---\n",
    "\n",
    "good_csv_path = 'good_data.csv'\n",
    "list_of_dataframes = []\n",
    "all_chunks_processed = False\n",
    "\n",
    "print(f\"\\nLoading good data from '{good_csv_path}' into DataFrame using chunks...\")\n",
    "chunk_iterator = None\n",
    "\n",
    "try:\n",
    "    # We use chunksize AND on_bad_lines='skip'\n",
    "    # 'skip' will automatically ignore line 25,432 (and any other errors)\n",
    "    chunk_iterator = pd.read_csv(\n",
    "        good_csv_path,\n",
    "        chunksize=1000000,\n",
    "        on_bad_lines='skip' # This is the fix\n",
    "    )\n",
    "    all_chunks_processed = True\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {good_csv_path} was not found.\")\n",
    "    sys.exit()\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Error: The CSV file is empty.\")\n",
    "    sys.exit()\n",
    "\n",
    "if all_chunks_processed:\n",
    "    for i, chunk in enumerate(chunk_iterator):\n",
    "        print(f\"Processing data chunk {i+1}...\")\n",
    "        if chunk.empty:\n",
    "            print(\"Chunk is empty, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Ensure the columns are what we expect, in case of a bad first chunk\n",
    "        if list(chunk.columns) != ['STATE_UT', 'DISTRICT', 'BLOCK', 'VILLAGE', 'LATITUDE', 'LONGITUDE', 'Date', 'WL(mbgl)']:\n",
    "             print(f\"Skipping chunk {i+1} due to malformed header.\")\n",
    "             continue\n",
    "\n",
    "        # Convert types\n",
    "        chunk['WL(mbgl)'] = pd.to_numeric(chunk['WL(mbgl)'], errors='coerce')\n",
    "        chunk['Date'] = pd.to_datetime(chunk['Date'], errors='coerce')\n",
    "        chunk['LATITUDE'] = pd.to_numeric(chunk['LATITUDE'], errors='coerce')\n",
    "        chunk['LONGITUDE'] = pd.to_numeric(chunk['LONGITUDE'], errors='coerce')\n",
    "\n",
    "        list_of_dataframes.append(chunk)\n",
    "\n",
    "# --- 4. Combine all chunks into the final DataFrame ---\n",
    "if list_of_dataframes:\n",
    "    print(\"Combining all processed chunks into one DataFrame...\")\n",
    "    df2 = pd.concat(list_of_dataframes)\n",
    "\n",
    "    print(\"\\n--- DataFrame is Ready for Analysis ---\")\n",
    "    print(df2.head())\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    df2.info()\n",
    "else:\n",
    "    print(\"No valid data chunks were loaded. The DataFrame is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75f29dac-70dd-4d71-8084-e37fe25f5280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE_UT</th>\n",
       "      <th>DISTRICT</th>\n",
       "      <th>BLOCK</th>\n",
       "      <th>VILLAGE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>Date</th>\n",
       "      <th>WL(mbgl)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147979</th>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>Fatehpur</td>\n",
       "      <td>Khajuha</td>\n",
       "      <td>Sarain Bakewar</td>\n",
       "      <td>26.11944</td>\n",
       "      <td>80.48333</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35611</th>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>Y.S.R.</td>\n",
       "      <td>Kamalapuram</td>\n",
       "      <td>Chadipiralla</td>\n",
       "      <td>14.58752</td>\n",
       "      <td>78.63683</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144777</th>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>Mahasamund</td>\n",
       "      <td>Mahasamund</td>\n",
       "      <td>Mahasamund</td>\n",
       "      <td>21.10833</td>\n",
       "      <td>82.09583</td>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93355</th>\n",
       "      <td>Karnataka</td>\n",
       "      <td>Koppal</td>\n",
       "      <td>Kukunoor</td>\n",
       "      <td>Itagi</td>\n",
       "      <td>15.44360</td>\n",
       "      <td>75.96860</td>\n",
       "      <td>2014-05-01</td>\n",
       "      <td>7.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18307</th>\n",
       "      <td>Gujarat</td>\n",
       "      <td>Rajkot</td>\n",
       "      <td>Rajkot</td>\n",
       "      <td>Sardhar</td>\n",
       "      <td>22.14583</td>\n",
       "      <td>70.98889</td>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196301</th>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>Hanumangarh</td>\n",
       "      <td>Nohar</td>\n",
       "      <td>Ramsara</td>\n",
       "      <td>29.25000</td>\n",
       "      <td>74.83330</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134409</th>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>Yavatmal</td>\n",
       "      <td>Ghatanji</td>\n",
       "      <td>Ghatanji</td>\n",
       "      <td>20.13333</td>\n",
       "      <td>78.31667</td>\n",
       "      <td>2006-10-01</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179605</th>\n",
       "      <td>Odisha</td>\n",
       "      <td>Bargarh</td>\n",
       "      <td>Bheden</td>\n",
       "      <td>Lupursinga</td>\n",
       "      <td>21.24028</td>\n",
       "      <td>83.79806</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175274</th>\n",
       "      <td>Haryana</td>\n",
       "      <td>Kurukshetra</td>\n",
       "      <td>Pehowa</td>\n",
       "      <td>Pehowa</td>\n",
       "      <td>29.98333</td>\n",
       "      <td>76.58056</td>\n",
       "      <td>1997-01-01</td>\n",
       "      <td>12.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44867</th>\n",
       "      <td>Kerala</td>\n",
       "      <td>Kasaragod</td>\n",
       "      <td>Kanhangad</td>\n",
       "      <td>Pallikkara</td>\n",
       "      <td>12.41450</td>\n",
       "      <td>75.05140</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106045</th>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>Balod</td>\n",
       "      <td>Gunderdehi</td>\n",
       "      <td>Arjunda</td>\n",
       "      <td>20.94306</td>\n",
       "      <td>81.20556</td>\n",
       "      <td>2012-01-19</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62225</th>\n",
       "      <td>Kerala</td>\n",
       "      <td>Palakkad</td>\n",
       "      <td>Chittur</td>\n",
       "      <td>Ellissery</td>\n",
       "      <td>10.75830</td>\n",
       "      <td>76.81440</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134878</th>\n",
       "      <td>Odisha</td>\n",
       "      <td>Jharsuguda</td>\n",
       "      <td>Lakhanpur</td>\n",
       "      <td>Bhikhampali</td>\n",
       "      <td>21.78694</td>\n",
       "      <td>83.57167</td>\n",
       "      <td>2006-01-31</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33064</th>\n",
       "      <td>Odisha</td>\n",
       "      <td>Mayurbhanj</td>\n",
       "      <td>Thakurmunda</td>\n",
       "      <td>Taramara</td>\n",
       "      <td>21.47194</td>\n",
       "      <td>86.15889</td>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32761</th>\n",
       "      <td>Odisha</td>\n",
       "      <td>Kendujhar</td>\n",
       "      <td>Bansapal</td>\n",
       "      <td>Kanjipani</td>\n",
       "      <td>21.49944</td>\n",
       "      <td>85.47639</td>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              STATE_UT     DISTRICT        BLOCK         VILLAGE  LATITUDE  \\\n",
       "147979   Uttar Pradesh     Fatehpur      Khajuha  Sarain Bakewar  26.11944   \n",
       "35611   Andhra Pradesh       Y.S.R.  Kamalapuram    Chadipiralla  14.58752   \n",
       "144777    Chhattisgarh   Mahasamund   Mahasamund      Mahasamund  21.10833   \n",
       "93355        Karnataka       Koppal     Kukunoor           Itagi  15.44360   \n",
       "18307          Gujarat       Rajkot       Rajkot         Sardhar  22.14583   \n",
       "196301       Rajasthan  Hanumangarh        Nohar         Ramsara  29.25000   \n",
       "134409     Maharashtra     Yavatmal     Ghatanji        Ghatanji  20.13333   \n",
       "179605          Odisha      Bargarh       Bheden      Lupursinga  21.24028   \n",
       "175274         Haryana  Kurukshetra       Pehowa          Pehowa  29.98333   \n",
       "44867           Kerala    Kasaragod    Kanhangad      Pallikkara  12.41450   \n",
       "106045    Chhattisgarh        Balod   Gunderdehi         Arjunda  20.94306   \n",
       "62225           Kerala     Palakkad      Chittur       Ellissery  10.75830   \n",
       "134878          Odisha   Jharsuguda    Lakhanpur     Bhikhampali  21.78694   \n",
       "33064           Odisha   Mayurbhanj  Thakurmunda        Taramara  21.47194   \n",
       "32761           Odisha    Kendujhar     Bansapal       Kanjipani  21.49944   \n",
       "\n",
       "        LONGITUDE       Date  WL(mbgl)  \n",
       "147979   80.48333 2003-01-01       2.1  \n",
       "35611    78.63683 2020-01-01       2.9  \n",
       "144777   82.09583 2004-10-01       6.2  \n",
       "93355    75.96860 2014-05-01       7.8  \n",
       "18307    70.98889 2023-10-01       2.5  \n",
       "196301   74.83330 1994-01-01      22.2  \n",
       "134409   78.31667 2006-10-01       6.3  \n",
       "179605   83.79806 1997-01-01       2.1  \n",
       "175274   76.58056 1997-01-01      12.7  \n",
       "44867    75.05140 2019-01-01      10.1  \n",
       "106045   81.20556 2012-01-19       2.0  \n",
       "62225    76.81440 2017-01-01       7.3  \n",
       "134878   83.57167 2006-01-31       4.0  \n",
       "33064    86.15889 2021-10-01       4.5  \n",
       "32761    85.47639 2021-10-01       3.6  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "100e9d85-a2db-46e3-baef-c5478b5621ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198368, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a32054be-75c1-4529-acbf-faea67866793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(383167, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e0d1eb9-0911-4d8d-a738-514fdc4d4b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening PDF: january_wl_1994-2024-compressed.pdf\n",
      "Total pages: 10242\n",
      "Processing page 500 / 10242\n",
      "Processing page 1000 / 10242\n",
      "Processing page 1500 / 10242\n",
      "Processing page 2000 / 10242\n",
      "Processing page 2500 / 10242\n",
      "Processing page 3000 / 10242\n",
      "Processing page 3500 / 10242\n",
      "Processing page 4000 / 10242\n",
      "Processing page 4500 / 10242\n",
      "Processing page 5000 / 10242\n",
      "Processing page 5500 / 10242\n",
      "Processing page 6000 / 10242\n",
      "Processing page 6500 / 10242\n",
      "Processing page 7000 / 10242\n",
      "Processing page 7500 / 10242\n",
      "Processing page 8000 / 10242\n",
      "Processing page 8500 / 10242\n",
      "Processing page 9000 / 10242\n",
      "Processing page 9500 / 10242\n",
      "Processing page 10000 / 10242\n",
      "Processing page 10242 / 10242\n",
      "\n",
      "--- Extraction Complete ---\n",
      "Good parent rows: 5325\n",
      "Child rows fixed: 937\n",
      "Total good rows: 6262\n",
      "Bad rows: 1534494\n",
      "PDF closed.\n",
      "\n",
      "Loading good_data_v4.csv into DataFrame...\n",
      "\n",
      " DataFrame Ready\n",
      "         STATE_UT               DISTRICT                          BLOCK  \\\n",
      "0  Andhra Pradesh  Alluri Sitharama Raju                    Addateegala   \n",
      "1  Andhra Pradesh  Alluri Sitharama Raju                    Addateegala   \n",
      "2  Andhra Pradesh              Annamayya  Peddathippasamudram Pulikallu   \n",
      "3  Andhra Pradesh              Annamayya                      Penagalur   \n",
      "4  Andhra Pradesh              Annamayya                      Penagalur   \n",
      "\n",
      "       VILLAGE  LATITUDE  LONGITUDE       Date  WL(mbgl)  \n",
      "0  Addateegala  17.46330   82.02710 2024-01-01       NaN  \n",
      "1  Addateegala  17.46330   82.02710 2024-01-01      13.4  \n",
      "2     13.77380  78.22435        NaN        NaT       NaN  \n",
      "3   Penagaluru  14.29502   79.26093 2024-01-01       NaN  \n",
      "4   Penagaluru  14.29502   79.26093 2024-01-01       4.3  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6262 entries, 0 to 6261\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   STATE_UT   6262 non-null   object        \n",
      " 1   DISTRICT   6262 non-null   object        \n",
      " 2   BLOCK      6262 non-null   object        \n",
      " 3   VILLAGE    6262 non-null   object        \n",
      " 4   LATITUDE   6248 non-null   float64       \n",
      " 5   LONGITUDE  1739 non-null   float64       \n",
      " 6   Date       1797 non-null   datetime64[ns]\n",
      " 7   WL(mbgl)   937 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(3), object(4)\n",
      "memory usage: 391.5+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\osadm\\AppData\\Local\\Temp\\ipykernel_3644\\2871025819.py:152: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['Date'] = pd.to_datetime(chunk['Date'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- 1. Helper Validation Functions ---\n",
    "\n",
    "def is_date(s):\n",
    "    \"\"\"Relaxed date check: accepts dd-mm-yy, dd-mm-yyyy, d-m-yy, etc.\"\"\"\n",
    "    s = s.strip()\n",
    "    return bool(re.match(r'^\\d{1,2}-\\d{1,2}-\\d{2,4}$', s)) or bool(re.match(r'^\\d{4}-\\d{1,2}-\\d{1,2}$', s))\n",
    "\n",
    "def is_number(s):\n",
    "    \"\"\"Relaxed numeric check (tolerates commas or misplaced dots).\"\"\"\n",
    "    s = s.strip().replace(',', '')\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# --- 2. Configuration ---\n",
    "\n",
    "pdf_path = path \n",
    "good_csv_path = \"good_data_v4.csv\"\n",
    "error_csv_path = \"error_data_v4.csv\"\n",
    "\n",
    "csv_header = \"STATE_UT,DISTRICT,BLOCK,VILLAGE,LATITUDE,LONGITUDE,Date,WL(mbgl)\"\n",
    "\n",
    "STATES_AND_UTS = {\n",
    "    'Andhra Pradesh', 'Arunachal Pradesh', 'Assam', 'Bihar', 'Chhattisgarh',\n",
    "    'Goa', 'Gujarat', 'Haryana', 'Himachal Pradesh', 'Jharkhand', 'Karnataka',\n",
    "    'Kerala', 'Madhya Pradesh', 'Maharashtra', 'Manipur', 'Meghalaya',\n",
    "    'Mizoram', 'Nagaland', 'Odisha', 'Punjab', 'Rajasthan', 'Sikkim',\n",
    "    'Tamil Nadu', 'Telangana', 'Tripura', 'Uttar Pradesh', 'Uttarakhand',\n",
    "    'West Bengal', 'Andaman and Nicobar Islands', 'Chandigarh',\n",
    "    'Dadra and Nagar Haveli and Daman and Diu', 'Delhi', 'Jammu and Kashmir',\n",
    "    'Ladakh', 'Lakshadweep', 'Puducherry'\n",
    "}\n",
    "\n",
    "junk_lines = {\n",
    "    'STATE_UT', 'DISTRICT', 'BLOCK', 'VILLAGE', 'LATITUDE', 'LONGITUDE',\n",
    "    'Date', 'WL(mbgl)', '',\n",
    "    'January month Depth to Water Level (In mbgl) Data of Unconfined Aquifer'\n",
    "}\n",
    "\n",
    "# --- 3. Main Extraction ---\n",
    "\n",
    "print(f\"Opening PDF: {pdf_path}\")\n",
    "doc = None\n",
    "\n",
    "good_rows_written = 0\n",
    "bad_rows_written = 0\n",
    "child_rows_fixed = 0\n",
    "\n",
    "last_known_location = []  # [STATE, DISTRICT, BLOCK, VILLAGE, LAT, LONG]\n",
    "temp_buffer = []\n",
    "\n",
    "try:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = doc.page_count\n",
    "    print(f\"Total pages: {total_pages}\")\n",
    "\n",
    "    with open(good_csv_path, 'w', encoding='utf-8') as f_good, \\\n",
    "         open(error_csv_path, 'w', encoding='utf-8') as f_error:\n",
    "\n",
    "        f_good.write(csv_header + '\\n')\n",
    "        f_error.write(\"REASON,DATA\\n\")\n",
    "\n",
    "        for i in range(total_pages):\n",
    "            if (i + 1) % 500 == 0 or (i + 1) == total_pages:\n",
    "                print(f\"Processing page {i + 1} / {total_pages}\")\n",
    "\n",
    "            page = doc.load_page(i)\n",
    "            text = page.get_text(\"text\")\n",
    "\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            lines = [ln.strip() for ln in text.split('\\n') if ln.strip()]\n",
    "\n",
    "            for line in lines:\n",
    "                if line in junk_lines:\n",
    "                    continue\n",
    "\n",
    "                # Auto-fix merged words like \"KonaseemaAinavilli\"\n",
    "                line = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', line)\n",
    "\n",
    "                temp_buffer.append(line)\n",
    "\n",
    "                # --- Case 1: starts with state (new parent row)\n",
    "                if temp_buffer[0] in STATES_AND_UTS:\n",
    "                    if 7 <= len(temp_buffer) <= 9:\n",
    "                        f_good.write(','.join(temp_buffer[:8]) + '\\n')\n",
    "                        last_known_location = temp_buffer[:6]\n",
    "                        temp_buffer = []\n",
    "                        good_rows_written += 1\n",
    "                    elif len(temp_buffer) > 9:\n",
    "                        f_error.write(f\"OVERFLOW_PARENT_ROW,{','.join(temp_buffer)}\\n\")\n",
    "                        temp_buffer = []\n",
    "                        bad_rows_written += 1\n",
    "\n",
    "                # --- Case 2: continuation (child rows)\n",
    "                else:\n",
    "                    if last_known_location and len(temp_buffer) == 2:\n",
    "                        a, b = temp_buffer\n",
    "                        if is_date(a) and is_number(b):\n",
    "                            f_good.write(','.join(last_known_location + temp_buffer) + '\\n')\n",
    "                            child_rows_fixed += 1\n",
    "                        else:\n",
    "                            f_error.write(f\"INVALID_CHILD_ROW,{','.join(temp_buffer)}\\n\")\n",
    "                            bad_rows_written += 1\n",
    "                        temp_buffer = []\n",
    "\n",
    "                    elif len(temp_buffer) > 2:\n",
    "                        # possible carry-over or overflow; try auto-correct\n",
    "                        joined = ' '.join(temp_buffer)\n",
    "                        tokens = joined.split()\n",
    "                        if len(tokens) >= 8:\n",
    "                            f_good.write(','.join(tokens[:8]) + '\\n')\n",
    "                            last_known_location = tokens[:6]\n",
    "                            good_rows_written += 1\n",
    "                        else:\n",
    "                            f_error.write(f\"SHORT_ROW,{','.join(tokens)}\\n\")\n",
    "                            bad_rows_written += 1\n",
    "                        temp_buffer = []\n",
    "\n",
    "    print(\"\\n--- Extraction Complete ---\")\n",
    "    print(f\"Good parent rows: {good_rows_written}\")\n",
    "    print(f\"Child rows fixed: {child_rows_fixed}\")\n",
    "    print(f\"Total good rows: {good_rows_written + child_rows_fixed}\")\n",
    "    print(f\"Bad rows: {bad_rows_written}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Extraction error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if doc:\n",
    "        doc.close()\n",
    "    print(\"PDF closed.\")\n",
    "\n",
    "# --- 4. Load and Clean ---\n",
    "\n",
    "if good_rows_written + child_rows_fixed > 0:\n",
    "    try:\n",
    "        print(f\"\\nLoading {good_csv_path} into DataFrame...\")\n",
    "        df_chunks = []\n",
    "        for chunk in pd.read_csv(good_csv_path, chunksize=1000000, on_bad_lines='skip'):\n",
    "            chunk['WL(mbgl)'] = pd.to_numeric(chunk['WL(mbgl)'], errors='coerce')\n",
    "            chunk['LATITUDE'] = pd.to_numeric(chunk['LATITUDE'], errors='coerce')\n",
    "            chunk['LONGITUDE'] = pd.to_numeric(chunk['LONGITUDE'], errors='coerce')\n",
    "            chunk['Date'] = pd.to_datetime(chunk['Date'], errors='coerce')\n",
    "            df_chunks.append(chunk)\n",
    "\n",
    "        if df_chunks:\n",
    "            df = pd.concat(df_chunks)\n",
    "            print(\"\\n DataFrame Ready\")\n",
    "            print(df.head())\n",
    "            print(df.info())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "else:\n",
    "    print(\"No good rows extracted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aec143-2dbf-4202-b964-686586995a9f",
   "metadata": {},
   "source": [
    "version 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "568232f9-64d0-418d-9f06-31535ed7f3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing PDF ...\n",
      "Total tokens: 3,572,403\n",
      "\n",
      "Good rows: 223,830\n",
      "Bad rows: 275,753\n",
      "\n",
      " Saved clean data to good_data_v5.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def is_date(tok):\n",
    "    return bool(re.match(r'^\\d{1,2}-\\d{1,2}-\\d{2,4}$', tok))\n",
    "\n",
    "def is_number(tok):\n",
    "    try:\n",
    "        float(tok.replace(',', ''))\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "pdf_path = path   # your PDF\n",
    "good_csv = \"good_data_v5.csv\"\n",
    "error_csv = \"error_data_v5.csv\"\n",
    "\n",
    "STATES = {\n",
    "    'Andhra Pradesh','Arunachal Pradesh','Assam','Bihar','Chhattisgarh','Goa',\n",
    "    'Gujarat','Haryana','Himachal Pradesh','Jharkhand','Karnataka','Kerala',\n",
    "    'Madhya Pradesh','Maharashtra','Manipur','Meghalaya','Mizoram','Nagaland',\n",
    "    'Odisha','Punjab','Rajasthan','Sikkim','Tamil Nadu','Telangana','Tripura',\n",
    "    'Uttar Pradesh','Uttarakhand','West Bengal','Andaman and Nicobar Islands',\n",
    "    'Chandigarh','Dadra and Nagar Haveli and Daman and Diu','Delhi',\n",
    "    'Jammu and Kashmir','Ladakh','Lakshadweep','Puducherry'\n",
    "}\n",
    "\n",
    "doc = fitz.open(pdf_path)\n",
    "tokens = []\n",
    "\n",
    "print(\"Tokenizing PDF ...\")\n",
    "for i in range(doc.page_count):\n",
    "    text = doc.load_page(i).get_text(\"text\")\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)   # fix merged words\n",
    "    tokens += text.split()\n",
    "\n",
    "rows = []\n",
    "bad_rows = 0\n",
    "current = []\n",
    "\n",
    "print(f\"Total tokens: {len(tokens):,}\")\n",
    "\n",
    "for tok in tokens:\n",
    "    # Start a new row when we hit a state name\n",
    "    if tok in STATES:\n",
    "        if len(current) == 8:\n",
    "            rows.append(current)\n",
    "        elif current:\n",
    "            bad_rows += 1\n",
    "        current = [tok]\n",
    "    else:\n",
    "        current.append(tok)\n",
    "        if len(current) == 8:\n",
    "            # basic sanity: lat/lon/date/wl\n",
    "            lat, lon, date, wl = current[4:8]\n",
    "            if is_number(lat) and is_number(lon) and is_date(date) and is_number(wl):\n",
    "                rows.append(current)\n",
    "            else:\n",
    "                bad_rows += 1\n",
    "            current = []\n",
    "\n",
    "# Flush tail\n",
    "if len(current) == 8:\n",
    "    rows.append(current)\n",
    "else:\n",
    "    bad_rows += 1\n",
    "\n",
    "print(f\"\\nGood rows: {len(rows):,}\")\n",
    "print(f\"Bad rows: {bad_rows:,}\")\n",
    "\n",
    "# Write to CSV\n",
    "pd.DataFrame(rows, columns=[\n",
    "    'STATE_UT','DISTRICT','BLOCK','VILLAGE','LATITUDE','LONGITUDE','Date','WL(mbgl)'\n",
    "]).to_csv(good_csv, index=False)\n",
    "\n",
    "print(f\"\\n Saved clean data to {good_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6df4a0-0523-47eb-94ed-d1e9a1a39564",
   "metadata": {},
   "source": [
    "v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "215be25b-2427-42a7-9c32-0cbe50057f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 3,449,989\n",
      "\n",
      "Good rows: 383,559 | Bad fragments: 65\n",
      " Saved clean data to good_data_v6.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- Helpers ---\n",
    "\n",
    "def is_date(tok):\n",
    "    tok = tok.strip()\n",
    "    # Accept 01-01-24, 01/01/2024, 01.01.24, 1-Jan-24, Jan-24\n",
    "    return bool(re.match(r'^(\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4})$', tok)) \\\n",
    "        or bool(re.match(r'^\\d{1,2}-[A-Za-z]{3,}-\\d{2,4}$', tok)) \\\n",
    "        or bool(re.match(r'^[A-Za-z]{3,}-\\d{2,4}$', tok))\n",
    "\n",
    "def is_number(tok):\n",
    "    try:\n",
    "        float(tok.replace(',', ''))\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def merge_multiword_states(text):\n",
    "    \"\"\"Rejoin known multi-word state names to single tokens.\"\"\"\n",
    "    replacements = [\n",
    "        (\"Andaman and Nicobar Islands\", \"Andaman_Nicobar_Islands\"),\n",
    "        (\"Dadra and Nagar Haveli and Daman and Diu\", \"Dadra_Nagar_Haveli_Daman_Diu\"),\n",
    "        (\"Jammu and Kashmir\", \"Jammu_Kashmir\"),\n",
    "        (\"Madhya Pradesh\", \"Madhya_Pradesh\"),\n",
    "        (\"Tamil Nadu\", \"Tamil_Nadu\"),\n",
    "        (\"Uttar Pradesh\", \"Uttar_Pradesh\"),\n",
    "        (\"West Bengal\", \"West_Bengal\"),\n",
    "        (\"Andhra Pradesh\", \"Andhra_Pradesh\"),\n",
    "    ]\n",
    "    for a,b in replacements:\n",
    "        text = text.replace(a, b)\n",
    "    return text\n",
    "\n",
    "# --- Config ---\n",
    "pdf_path = path\n",
    "good_csv = \"good_data_v6.csv\"\n",
    "\n",
    "STATES = {\n",
    "    'Andhra_Pradesh','Arunachal Pradesh','Assam','Bihar','Chhattisgarh','Goa',\n",
    "    'Gujarat','Haryana','Himachal Pradesh','Jharkhand','Karnataka','Kerala',\n",
    "    'Madhya_Pradesh','Maharashtra','Manipur','Meghalaya','Mizoram','Nagaland',\n",
    "    'Odisha','Punjab','Rajasthan','Sikkim','Tamil_Nadu','Telangana','Tripura',\n",
    "    'Uttar_Pradesh','Uttarakhand','West_Bengal',\n",
    "    'Andaman_Nicobar_Islands','Chandigarh',\n",
    "    'Dadra_Nagar_Haveli_Daman_Diu','Delhi',\n",
    "    'Jammu_Kashmir','Ladakh','Lakshadweep','Puducherry'\n",
    "}\n",
    "\n",
    "# --- 1. Tokenize ---\n",
    "doc = fitz.open(pdf_path)\n",
    "tokens = []\n",
    "for i in range(doc.page_count):\n",
    "    text = doc.load_page(i).get_text(\"text\")\n",
    "    text = merge_multiword_states(text)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    tokens += text.split()\n",
    "\n",
    "print(f\"Total tokens: {len(tokens):,}\")\n",
    "\n",
    "# --- 2. Scan tokens with sliding window ---\n",
    "rows = []\n",
    "bad = 0\n",
    "current = []\n",
    "\n",
    "for idx, tok in enumerate(tokens):\n",
    "    if tok in STATES:\n",
    "        # flush old row if valid\n",
    "        if len(current) == 8:\n",
    "            rows.append(current)\n",
    "        current = [tok]\n",
    "        continue\n",
    "\n",
    "    if not current:\n",
    "        continue\n",
    "\n",
    "    current.append(tok)\n",
    "\n",
    "    # When buffer is between 812 tokens, test every 8-token slice\n",
    "    if len(current) >= 8:\n",
    "        found = False\n",
    "        for start in range(max(0, len(current)-12), len(current)-7):\n",
    "            slice8 = current[start:start+8]\n",
    "            lat, lon, date, wl = slice8[4:8]\n",
    "            if is_number(lat) and is_number(lon) and is_date(date) and is_number(wl):\n",
    "                rows.append(slice8)\n",
    "                found = True\n",
    "                current = []\n",
    "                break\n",
    "        if not found and len(current) > 12:\n",
    "            bad += 1\n",
    "            current = []\n",
    "\n",
    "# --- 3. Save ---\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    'STATE_UT','DISTRICT','BLOCK','VILLAGE','LATITUDE','LONGITUDE','Date','WL(mbgl)'\n",
    "])\n",
    "print(f\"\\nGood rows: {len(df):,} | Bad fragments: {bad:,}\")\n",
    "df.to_csv(good_csv, index=False)\n",
    "print(f\" Saved clean data to {good_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af489792-da1f-4f86-a3f6-4d88e3689ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 3,209,238\n",
      "\n",
      "Good rows: 353,246 | Bad fragments: 55\n",
      " Saved clean data to good_data_v6_aug.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- Helpers ---\n",
    "\n",
    "def is_date(tok):\n",
    "    tok = tok.strip()\n",
    "    # Accept 01-01-24, 01/01/2024, 01.01.24, 1-Jan-24, Jan-24\n",
    "    return bool(re.match(r'^(\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4})$', tok)) \\\n",
    "        or bool(re.match(r'^\\d{1,2}-[A-Za-z]{3,}-\\d{2,4}$', tok)) \\\n",
    "        or bool(re.match(r'^[A-Za-z]{3,}-\\d{2,4}$', tok))\n",
    "\n",
    "def is_number(tok):\n",
    "    try:\n",
    "        float(tok.replace(',', ''))\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def merge_multiword_states(text):\n",
    "    \"\"\"Rejoin known multi-word state names to single tokens.\"\"\"\n",
    "    replacements = [\n",
    "        (\"Andaman and Nicobar Islands\", \"Andaman_Nicobar_Islands\"),\n",
    "        (\"Dadra and Nagar Haveli and Daman and Diu\", \"Dadra_Nagar_Haveli_Daman_Diu\"),\n",
    "        (\"Jammu and Kashmir\", \"Jammu_Kashmir\"),\n",
    "        (\"Madhya Pradesh\", \"Madhya_Pradesh\"),\n",
    "        (\"Tamil Nadu\", \"Tamil_Nadu\"),\n",
    "        (\"Uttar Pradesh\", \"Uttar_Pradesh\"),\n",
    "        (\"West Bengal\", \"West_Bengal\"),\n",
    "        (\"Andhra Pradesh\", \"Andhra_Pradesh\"),\n",
    "    ]\n",
    "    for a,b in replacements:\n",
    "        text = text.replace(a, b)\n",
    "    return text\n",
    "\n",
    "# --- Config ---\n",
    "pdf_path = \"august_wl_1994-2023_compressed.pdf\"\n",
    "good_csv = \"good_data_v6_aug.csv\"\n",
    "\n",
    "STATES = {\n",
    "    'Andhra_Pradesh','Arunachal Pradesh','Assam','Bihar','Chhattisgarh','Goa',\n",
    "    'Gujarat','Haryana','Himachal Pradesh','Jharkhand','Karnataka','Kerala',\n",
    "    'Madhya_Pradesh','Maharashtra','Manipur','Meghalaya','Mizoram','Nagaland',\n",
    "    'Odisha','Punjab','Rajasthan','Sikkim','Tamil_Nadu','Telangana','Tripura',\n",
    "    'Uttar_Pradesh','Uttarakhand','West_Bengal',\n",
    "    'Andaman_Nicobar_Islands','Chandigarh',\n",
    "    'Dadra_Nagar_Haveli_Daman_Diu','Delhi',\n",
    "    'Jammu_Kashmir','Ladakh','Lakshadweep','Puducherry'\n",
    "}\n",
    "\n",
    "# --- 1. Tokenize ---\n",
    "doc = fitz.open(pdf_path)\n",
    "tokens = []\n",
    "for i in range(doc.page_count):\n",
    "    text = doc.load_page(i).get_text(\"text\")\n",
    "    text = merge_multiword_states(text)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    tokens += text.split()\n",
    "\n",
    "print(f\"Total tokens: {len(tokens):,}\")\n",
    "\n",
    "# --- 2. Scan tokens with sliding window ---\n",
    "rows = []\n",
    "bad = 0\n",
    "current = []\n",
    "\n",
    "for idx, tok in enumerate(tokens):\n",
    "    if tok in STATES:\n",
    "        # flush old row if valid\n",
    "        if len(current) == 8:\n",
    "            rows.append(current)\n",
    "        current = [tok]\n",
    "        continue\n",
    "\n",
    "    if not current:\n",
    "        continue\n",
    "\n",
    "    current.append(tok)\n",
    "\n",
    "    # When buffer is between 812 tokens, test every 8-token slice\n",
    "    if len(current) >= 8:\n",
    "        found = False\n",
    "        for start in range(max(0, len(current)-12), len(current)-7):\n",
    "            slice8 = current[start:start+8]\n",
    "            lat, lon, date, wl = slice8[4:8]\n",
    "            if is_number(lat) and is_number(lon) and is_date(date) and is_number(wl):\n",
    "                rows.append(slice8)\n",
    "                found = True\n",
    "                current = []\n",
    "                break\n",
    "        if not found and len(current) > 12:\n",
    "            bad += 1\n",
    "            current = []\n",
    "\n",
    "# --- 3. Save ---\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    'STATE_UT','DISTRICT','BLOCK','VILLAGE','LATITUDE','LONGITUDE','Date','WL(mbgl)'\n",
    "])\n",
    "print(f\"\\nGood rows: {len(df):,} | Bad fragments: {bad:,}\")\n",
    "df.to_csv(good_csv, index=False)\n",
    "print(f\" Saved clean data to {good_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebd2316-7912-402a-88cd-43f0a68abe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 999,114\n",
      "\n",
      "Good rows: 111,313 | Bad fragments: 23\n",
      " Saved clean data to good_data_v6_pre1.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- Helpers ---\n",
    "\n",
    "def is_date(tok):\n",
    "    tok = tok.strip()\n",
    "    # Accept 01-01-24, 01/01/2024, 01.01.24, 1-Jan-24, Jan-24\n",
    "    return bool(re.match(r'^(\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4})$', tok)) \\\n",
    "        or bool(re.match(r'^\\d{1,2}-[A-Za-z]{3,}-\\d{2,4}$', tok)) \\\n",
    "        or bool(re.match(r'^[A-Za-z]{3,}-\\d{2,4}$', tok))\n",
    "\n",
    "def is_number(tok):\n",
    "    try:\n",
    "        float(tok.replace(',', ''))\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def merge_multiword_states(text):\n",
    "    \"\"\"Rejoin known multi-word state names to single tokens.\"\"\"\n",
    "    replacements = [\n",
    "        (\"Andaman and Nicobar Islands\", \"Andaman_Nicobar_Islands\"),\n",
    "        (\"Dadra and Nagar Haveli and Daman and Diu\", \"Dadra_Nagar_Haveli_Daman_Diu\"),\n",
    "        (\"Jammu and Kashmir\", \"Jammu_Kashmir\"),\n",
    "        (\"Madhya Pradesh\", \"Madhya_Pradesh\"),\n",
    "        (\"Tamil Nadu\", \"Tamil_Nadu\"),\n",
    "        (\"Uttar Pradesh\", \"Uttar_Pradesh\"),\n",
    "        (\"West Bengal\", \"West_Bengal\"),\n",
    "        (\"Andhra Pradesh\", \"Andhra_Pradesh\"),\n",
    "    ]\n",
    "    for a,b in replacements:\n",
    "        text = text.replace(a, b)\n",
    "    return text\n",
    "\n",
    "# --- Config ---\n",
    "pdf_path = \"pre-monsoon_1994-2003.pdf\"\n",
    "good_csv = \"good_data_v6_pre1.csv\"\n",
    "\n",
    "STATES = {\n",
    "    'Andhra_Pradesh','Arunachal Pradesh','Assam','Bihar','Chhattisgarh','Goa',\n",
    "    'Gujarat','Haryana','Himachal Pradesh','Jharkhand','Karnataka','Kerala',\n",
    "    'Madhya_Pradesh','Maharashtra','Manipur','Meghalaya','Mizoram','Nagaland',\n",
    "    'Odisha','Punjab','Rajasthan','Sikkim','Tamil_Nadu','Telangana','Tripura',\n",
    "    'Uttar_Pradesh','Uttarakhand','West_Bengal',\n",
    "    'Andaman_Nicobar_Islands','Chandigarh',\n",
    "    'Dadra_Nagar_Haveli_Daman_Diu','Delhi',\n",
    "    'Jammu_Kashmir','Ladakh','Lakshadweep','Puducherry'\n",
    "}\n",
    "\n",
    "# --- 1. Tokenize ---\n",
    "doc = fitz.open(pdf_path)\n",
    "tokens = []\n",
    "for i in range(doc.page_count):\n",
    "    text = doc.load_page(i).get_text(\"text\")\n",
    "    text = merge_multiword_states(text)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    tokens += text.split()\n",
    "\n",
    "print(f\"Total tokens: {len(tokens):,}\")\n",
    "\n",
    "# --- 2. Scan tokens with sliding window ---\n",
    "rows = []\n",
    "bad = 0\n",
    "current = []\n",
    "\n",
    "for idx, tok in enumerate(tokens):\n",
    "    if tok in STATES:\n",
    "        # flush old row if valid\n",
    "        if len(current) == 8:\n",
    "            rows.append(current)\n",
    "        current = [tok]\n",
    "        continue\n",
    "\n",
    "    if not current:\n",
    "        continue\n",
    "\n",
    "    current.append(tok)\n",
    "\n",
    "    # When buffer is between 812 tokens, test every 8-token slice\n",
    "    if len(current) >= 8:\n",
    "        found = False\n",
    "        for start in range(max(0, len(current)-12), len(current)-7):\n",
    "            slice8 = current[start:start+8]\n",
    "            lat, lon, date, wl = slice8[4:8]\n",
    "            if is_number(lat) and is_number(lon) and is_date(date) and is_number(wl):\n",
    "                rows.append(slice8)\n",
    "                found = True\n",
    "                current = []\n",
    "                break\n",
    "        if not found and len(current) > 12:\n",
    "            bad += 1\n",
    "            current = []\n",
    "\n",
    "# --- 3. Save ---\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    'STATE_UT','DISTRICT','BLOCK','VILLAGE','LATITUDE','LONGITUDE','Date','WL(mbgl)'\n",
    "])\n",
    "print(f\"\\nGood rows: {len(df):,} | Bad fragments: {bad:,}\")\n",
    "df.to_csv(good_csv, index=False)\n",
    "print(f\" Saved clean data to {good_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ee1f3e-2837-440c-85b0-369e235e63dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1,070,014\n",
      "\n",
      "Good rows: 117,671 | Bad fragments: 21\n",
      " Saved clean data to good_data_v6_pre2.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- Helpers ---\n",
    "\n",
    "def is_date(tok):\n",
    "    tok = tok.strip()\n",
    "    # Accept 01-01-24, 01/01/2024, 01.01.24, 1-Jan-24, Jan-24\n",
    "    return bool(re.match(r'^(\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4})$', tok)) \\\n",
    "        or bool(re.match(r'^\\d{1,2}-[A-Za-z]{3,}-\\d{2,4}$', tok)) \\\n",
    "        or bool(re.match(r'^[A-Za-z]{3,}-\\d{2,4}$', tok))\n",
    "\n",
    "def is_number(tok):\n",
    "    try:\n",
    "        float(tok.replace(',', ''))\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def merge_multiword_states(text):\n",
    "    \"\"\"Rejoin known multi-word state names to single tokens.\"\"\"\n",
    "    replacements = [\n",
    "        (\"Andaman and Nicobar Islands\", \"Andaman_Nicobar_Islands\"),\n",
    "        (\"Dadra and Nagar Haveli and Daman and Diu\", \"Dadra_Nagar_Haveli_Daman_Diu\"),\n",
    "        (\"Jammu and Kashmir\", \"Jammu_Kashmir\"),\n",
    "        (\"Madhya Pradesh\", \"Madhya_Pradesh\"),\n",
    "        (\"Tamil Nadu\", \"Tamil_Nadu\"),\n",
    "        (\"Uttar Pradesh\", \"Uttar_Pradesh\"),\n",
    "        (\"West Bengal\", \"West_Bengal\"),\n",
    "        (\"Andhra Pradesh\", \"Andhra_Pradesh\"),\n",
    "    ]\n",
    "    for a,b in replacements:\n",
    "        text = text.replace(a, b)\n",
    "    return text\n",
    "\n",
    "# --- Config ---\n",
    "pdf_path = \"pre-monsoon_2004-2013.pdf\"\n",
    "good_csv = \"good_data_v6_pre2.csv\"\n",
    "\n",
    "STATES = {\n",
    "    'Andhra_Pradesh','Arunachal Pradesh','Assam','Bihar','Chhattisgarh','Goa',\n",
    "    'Gujarat','Haryana','Himachal Pradesh','Jharkhand','Karnataka','Kerala',\n",
    "    'Madhya_Pradesh','Maharashtra','Manipur','Meghalaya','Mizoram','Nagaland',\n",
    "    'Odisha','Punjab','Rajasthan','Sikkim','Tamil_Nadu','Telangana','Tripura',\n",
    "    'Uttar_Pradesh','Uttarakhand','West_Bengal',\n",
    "    'Andaman_Nicobar_Islands','Chandigarh',\n",
    "    'Dadra_Nagar_Haveli_Daman_Diu','Delhi',\n",
    "    'Jammu_Kashmir','Ladakh','Lakshadweep','Puducherry'\n",
    "}\n",
    "\n",
    "# --- 1. Tokenize ---\n",
    "doc = fitz.open(pdf_path)\n",
    "tokens = []\n",
    "for i in range(doc.page_count):\n",
    "    text = doc.load_page(i).get_text(\"text\")\n",
    "    text = merge_multiword_states(text)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    tokens += text.split()\n",
    "\n",
    "print(f\"Total tokens: {len(tokens):,}\")\n",
    "\n",
    "# --- 2. Scan tokens with sliding window ---\n",
    "rows = []\n",
    "bad = 0\n",
    "current = []\n",
    "\n",
    "for idx, tok in enumerate(tokens):\n",
    "    if tok in STATES:\n",
    "        # flush old row if valid\n",
    "        if len(current) == 8:\n",
    "            rows.append(current)\n",
    "        current = [tok]\n",
    "        continue\n",
    "\n",
    "    if not current:\n",
    "        continue\n",
    "\n",
    "    current.append(tok)\n",
    "\n",
    "    # When buffer is between 812 tokens, test every 8-token slice\n",
    "    if len(current) >= 8:\n",
    "        found = False\n",
    "        for start in range(max(0, len(current)-12), len(current)-7):\n",
    "            slice8 = current[start:start+8]\n",
    "            lat, lon, date, wl = slice8[4:8]\n",
    "            if is_number(lat) and is_number(lon) and is_date(date) and is_number(wl):\n",
    "                rows.append(slice8)\n",
    "                found = True\n",
    "                current = []\n",
    "                break\n",
    "        if not found and len(current) > 12:\n",
    "            bad += 1\n",
    "            current = []\n",
    "\n",
    "# --- 3. Save ---\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    'STATE_UT','DISTRICT','BLOCK','VILLAGE','LATITUDE','LONGITUDE','Date','WL(mbgl)'\n",
    "])\n",
    "print(f\"\\nGood rows: {len(df):,} | Bad fragments: {bad:,}\")\n",
    "df.to_csv(good_csv, index=False)\n",
    "print(f\" Saved clean data to {good_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6601189-3c8c-4c32-9d91-2e66ce22e92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1,764,890\n",
      "\n",
      "Good rows: 192,839 | Bad fragments: 47\n",
      " Saved clean data to good_data_v6_pre3.csv\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# --- Helpers ---\n",
    "\n",
    "def is_date(tok):\n",
    "    tok = tok.strip()\n",
    "    # Accept 01-01-24, 01/01/2024, 01.01.24, 1-Jan-24, Jan-24\n",
    "    return bool(re.match(r'^(\\d{1,2}[-/.]\\d{1,2}[-/.]\\d{2,4})$', tok)) \\\n",
    "        or bool(re.match(r'^\\d{1,2}-[A-Za-z]{3,}-\\d{2,4}$', tok)) \\\n",
    "        or bool(re.match(r'^[A-Za-z]{3,}-\\d{2,4}$', tok))\n",
    "\n",
    "def is_number(tok):\n",
    "    try:\n",
    "        float(tok.replace(',', ''))\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def merge_multiword_states(text):\n",
    "    \"\"\"Rejoin known multi-word state names to single tokens.\"\"\"\n",
    "    replacements = [\n",
    "        (\"Andaman and Nicobar Islands\", \"Andaman_Nicobar_Islands\"),\n",
    "        (\"Dadra and Nagar Haveli and Daman and Diu\", \"Dadra_Nagar_Haveli_Daman_Diu\"),\n",
    "        (\"Jammu and Kashmir\", \"Jammu_Kashmir\"),\n",
    "        (\"Madhya Pradesh\", \"Madhya_Pradesh\"),\n",
    "        (\"Tamil Nadu\", \"Tamil_Nadu\"),\n",
    "        (\"Uttar Pradesh\", \"Uttar_Pradesh\"),\n",
    "        (\"West Bengal\", \"West_Bengal\"),\n",
    "        (\"Andhra Pradesh\", \"Andhra_Pradesh\"),\n",
    "    ]\n",
    "    for a,b in replacements:\n",
    "        text = text.replace(a, b)\n",
    "    return text\n",
    "\n",
    "# --- Config ---\n",
    "pdf_path = \"pre-monsoon_2014-2024.pdf\"\n",
    "good_csv = \"good_data_v6_pre3.csv\"\n",
    "\n",
    "STATES = {\n",
    "    'Andhra_Pradesh','Arunachal Pradesh','Assam','Bihar','Chhattisgarh','Goa',\n",
    "    'Gujarat','Haryana','Himachal Pradesh','Jharkhand','Karnataka','Kerala',\n",
    "    'Madhya_Pradesh','Maharashtra','Manipur','Meghalaya','Mizoram','Nagaland',\n",
    "    'Odisha','Punjab','Rajasthan','Sikkim','Tamil_Nadu','Telangana','Tripura',\n",
    "    'Uttar_Pradesh','Uttarakhand','West_Bengal',\n",
    "    'Andaman_Nicobar_Islands','Chandigarh',\n",
    "    'Dadra_Nagar_Haveli_Daman_Diu','Delhi',\n",
    "    'Jammu_Kashmir','Ladakh','Lakshadweep','Puducherry'\n",
    "}\n",
    "\n",
    "# --- 1. Tokenize ---\n",
    "doc = fitz.open(pdf_path)\n",
    "tokens = []\n",
    "for i in range(doc.page_count):\n",
    "    text = doc.load_page(i).get_text(\"text\")\n",
    "    text = merge_multiword_states(text)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    tokens += text.split()\n",
    "\n",
    "print(f\"Total tokens: {len(tokens):,}\")\n",
    "\n",
    "# --- 2. Scan tokens with sliding window ---\n",
    "rows = []\n",
    "bad = 0\n",
    "current = []\n",
    "\n",
    "for idx, tok in enumerate(tokens):\n",
    "    if tok in STATES:\n",
    "        # flush old row if valid\n",
    "        if len(current) == 8:\n",
    "            rows.append(current)\n",
    "        current = [tok]\n",
    "        continue\n",
    "\n",
    "    if not current:\n",
    "        continue\n",
    "\n",
    "    current.append(tok)\n",
    "\n",
    "    # When buffer is between 812 tokens, test every 8-token slice\n",
    "    if len(current) >= 8:\n",
    "        found = False\n",
    "        for start in range(max(0, len(current)-12), len(current)-7):\n",
    "            slice8 = current[start:start+8]\n",
    "            lat, lon, date, wl = slice8[4:8]\n",
    "            if is_number(lat) and is_number(lon) and is_date(date) and is_number(wl):\n",
    "                rows.append(slice8)\n",
    "                found = True\n",
    "                current = []\n",
    "                break\n",
    "        if not found and len(current) > 12:\n",
    "            bad += 1\n",
    "            current = []\n",
    "\n",
    "# --- 3. Save ---\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    'STATE_UT','DISTRICT','BLOCK','VILLAGE','LATITUDE','LONGITUDE','Date','WL(mbgl)'\n",
    "])\n",
    "print(f\"\\nGood rows: {len(df):,} | Bad fragments: {bad:,}\")\n",
    "df.to_csv(good_csv, index=False)\n",
    "print(f\" Saved clean data to {good_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbb660-fa05-4972-91ac-77494b724f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
